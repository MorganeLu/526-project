import shap
import numpy as np
import matplotlib.pyplot as plt
import os
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix
)
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from imblearn.over_sampling import SMOTE


# LGB
def train_lightgbm_models(subsets, X_test):
    test_preds = []  # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
    models = []      # å­˜å‚¨æ¨¡å‹æœ¬èº«

    for i, (X_train, y_train) in enumerate(subsets):
        print(f"\n =========================================Start training LightGBM model {i+1}=======================================")

        # åˆå§‹åŒ– LightGBM æ¨¡å‹
        model = lgb.LGBMClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=7,
            num_leaves=31,
            objective="binary"
        )

        # æ‰§è¡Œ 5 æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
        print(f" model {i+1} - AUC-ROC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # ç”¨å®Œæ•´å­é›†æ•°æ®é‡æ–°è®­ç»ƒ
        model.fit(X_train, y_train)
        models.append(model)

        # åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ï¼ˆæ­£ç±»æ¦‚ç‡ï¼‰
        proba = model.predict_proba(X_test)[:, 1]
        test_preds.append(proba)

    return models, test_preds


# Logistic Regression
def train_logistic_models(subsets, X_test):
    test_preds = []
    models = []
    scalers = []

    for i, (X_train, y_train) in enumerate(subsets):
        print(f"\n è®­ç»ƒ L2 Logistic Regression æ¨¡å‹ {i+1}...")

        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # åˆ›å»ºæ¨¡å‹ï¼ˆL2 æ­£åˆ™ï¼‰
        model = LogisticRegression(
            penalty='l2',
            C=1.0,
            solver='liblinear',  # æ”¯æŒ L1/L2 ä¸”ç¨³å®š
            max_iter=1000
        )

        # äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
        print(f" æ¨¡å‹ {i+1} çš„ 5 æŠ˜ AUC-ROC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # æ‹Ÿåˆå¹¶é¢„æµ‹
        model.fit(X_train_scaled, y_train)
        proba = model.predict_proba(X_test_scaled)[:, 1]

        models.append(model)
        scalers.append(scaler)
        test_preds.append(proba)

    return models, scalers, test_preds

def train_logistic_l2_models(subsets, X_test):
    test_preds = []
    models = []
    scalers = []

    for i, (X_train, y_train) in enumerate(subsets):
        print(f"\nğŸ”„ è®­ç»ƒ L2 Logistic Regression æ¨¡å‹ {i+1}ï¼ˆæ—  SMOTEï¼‰")

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # æ„å»ºæ¨¡å‹
        model = LogisticRegression(
            penalty='l2',
            C=1.0,
            solver='liblinear',
            max_iter=1000
        )

        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
        print(f"âœ… L2 æ¨¡å‹ {i+1} AUC-ROC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
        model.fit(X_train_scaled, y_train)
        proba = model.predict_proba(X_test_scaled)[:, 1]

        models.append(model)
        scalers.append(scaler)
        test_preds.append(proba)

    return models, scalers, test_preds

def train_logistic_l1_models(subsets, X_test):
    test_preds = []
    models = []
    scalers = []

    for i, (X_train, y_train) in enumerate(subsets):
        print(f"\nè®­ç»ƒ L1 Logistic Regression æ¨¡å‹ {i+1}ï¼ˆæ—  SMOTEï¼‰")

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # æ„å»ºæ¨¡å‹
        model = LogisticRegression(
            penalty='l1',
            C=1.0,
            solver='liblinear',
            max_iter=1000
        )

        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
        print(f"L1 æ¨¡å‹ {i+1} AUC-ROC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
        model.fit(X_train_scaled, y_train)
        proba = model.predict_proba(X_test_scaled)[:, 1]

        models.append(model)
        scalers.append(scaler)
        test_preds.append(proba)

    return models, scalers, test_preds


def train_logistic_models_with_smote(subsets, X_test):
    test_preds = []
    models = []
    scalers = []

    for i, (X_train, y_train) in enumerate(subsets):
        print(f"\nè®­ç»ƒ L2 Logistic Regression æ¨¡å‹ {i+1}ï¼ˆä½¿ç”¨ SMOTEï¼‰")

        # ä½¿ç”¨ SMOTE è¿‡é‡‡æ ·
        smote = SMOTE(random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_resampled_scaled = scaler.fit_transform(X_resampled)
        X_test_scaled = scaler.transform(X_test)

        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        model = LogisticRegression(
            penalty='l2',
            C=1.0,
            solver='liblinear',
            max_iter=1000
        )

        # äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = cross_val_score(model, X_resampled_scaled, y_resampled, cv=5, scoring='roc_auc')
        print(f"æ¨¡å‹ {i+1} çš„ 5 æŠ˜ AUC-ROC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # æ¨¡å‹è®­ç»ƒ + æµ‹è¯•é›†é¢„æµ‹
        model.fit(X_resampled_scaled, y_resampled)
        proba = model.predict_proba(X_test_scaled)[:, 1]

        models.append(model)
        scalers.append(scaler)
        test_preds.append(proba)

    return models, scalers, test_preds


# Random Forest
def train_random_forest_models(subsets, X_test):
    test_preds = []
    models = []

    for i, (X_train, y_train) in enumerate(subsets):
        print(f"\nè®­ç»ƒ Random Forest æ¨¡å‹ {i+1}(default params)")

        # æ„å»ºæ¨¡å‹ï¼ˆé»˜è®¤å‚æ•°ï¼‰
        model = RandomForestClassifier(
            random_state=42,
            n_jobs=-1
        )

        # äº¤å‰éªŒè¯ï¼ˆ5æŠ˜ AUC-ROCï¼‰
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
        print(f"æ¨¡å‹ {i+1} AUC-ROC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # è®­ç»ƒæ¨¡å‹ + åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹æ¦‚ç‡
        model.fit(X_train, y_train)
        proba = model.predict_proba(X_test)[:, 1]

        models.append(model)
        test_preds.append(proba)

    return models, test_preds


# general
def majority_vote_from_probs(prob_list, threshold=0.5):
    # æŠŠæ¦‚ç‡è½¬ä¸º 0/1
    binary_preds = [pred > threshold for pred in prob_list]

    # æ±‚æ¯ä¸ªæ ·æœ¬åœ¨3ä¸ªæ¨¡å‹ä¸­çš„å¹³å‡é¢„æµ‹ï¼ˆ0ï½1ï¼‰
    avg_preds = np.mean(binary_preds, axis=0)

    # å¤šæ•°æŠ•ç¥¨ï¼ˆ0.5 ä»¥ä¸Šä¸º 1ï¼‰
    final_preds = np.round(avg_preds).astype(int)

    return final_preds


def shap_summary_ensemble(models, X_test, output_dir="output"):
    print("==========================================Calculating SHAP value for each model===============================")

    shap_values_list = []
    for i, model in enumerate(models):
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test, check_additivity=False)

        # å¦‚æœæ˜¯ binary åˆ†ç±»ï¼Œå– class 1 çš„ shap å€¼
        if isinstance(shap_values, list):
            shap_values = shap_values[1]

        shap_values_list.append(shap_values)


    # å¯¹å¤šä¸ª shap_values å–å¹³å‡
    shap_values_avg = np.mean(np.array(shap_values_list), axis=0)

    os.makedirs(output_dir, exist_ok=True)

    plt.figure(figsize=(12, 6))
    shap.summary_plot(
        shap_values_avg,
        X_test,
        plot_type="dot",
        max_display=20,
        show=False
    )
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "lgb_ensemble_shap_summary.png"))
    plt.close()

    plt.figure(figsize=(12, 6))
    shap.summary_plot(
        shap_values_avg,
        X_test,
        plot_type="bar",
        max_display=20,
        show=False
    )
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "lgb_ensemble_shap_feature_importance.png"))
    plt.close()

    print(f" SHAP summary saved to {output_dir}/")